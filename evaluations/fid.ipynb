{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3Dataset(Dataset):\n",
    "    def __init__(self, images_path, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.device = device\n",
    "\n",
    "        self.image_files = os.listdir(images_path)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.CenterCrop(299),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(osp.join(self.images_path, self.image_files[idx]))\n",
    "        image_tensor = self.transform(image).to(self.device)\n",
    "\n",
    "        return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kjk/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception v3 model loaded\n"
     ]
    }
   ],
   "source": [
    "inception_v3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', weights='IMAGENET1K_V1')\n",
    "\n",
    "inception_v3.to('cuda')\n",
    "inception_v3.eval()\n",
    "\n",
    "print('inception v3 model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace last two layers of inception_v3\n",
    "class ID(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ID, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "inception_v3.dropout = ID()\n",
    "inception_v3.fc = ID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of output images: 2032\n",
      "# of gt images: 2032\n"
     ]
    }
   ],
   "source": [
    "#user input\n",
    "output_dir = '../inference/20250131_Base/999epoch/repaint/pair/'\n",
    "gt_dir = '../DATA/VITON-HD/test/image/'\n",
    "#user input\n",
    "\n",
    "output_dataset = InceptionV3Dataset(output_dir)\n",
    "gt_dataset = InceptionV3Dataset(gt_dir)\n",
    "\n",
    "print('# of output images:', len(output_dataset))\n",
    "print('# of gt images:', len(gt_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output batches: 100%|██████████| 4/4 [00:12<00:00,  3.01s/it]\n",
      "Processing ground truth batches: 100%|██████████| 4/4 [00:24<00:00,  6.03s/it]\n"
     ]
    }
   ],
   "source": [
    "output_dataloader = DataLoader(dataset=output_dataset, batch_size=512)\n",
    "gt_dataloader = DataLoader(dataset=gt_dataset, batch_size=512)\n",
    "\n",
    "output_activation = []\n",
    "gt_activation = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(output_dataloader, desc=\"Processing output batches\"):\n",
    "        act = inception_v3(batch).detach().cpu().numpy()\n",
    "        output_activation.append(act)\n",
    "    for batch in tqdm(gt_dataloader, desc=\"Processing ground truth batches\"):\n",
    "        act = inception_v3(batch).detach().cpu().numpy()\n",
    "        gt_activation.append(act)\n",
    "\n",
    "output_activation = np.vstack(output_activation)\n",
    "gt_activation = np.vstack(gt_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FID 점수는 두 가지 요소로 구성됩니다: 첫 번째는 평균 벡터의 차이이고, 두 번째는 공분산 행렬의 차이입니다. 수학적으로 FID는 다음과 같은 수식으로 표현됩니다:\n",
    "\n",
    "$ FID(x, g) = ||μ_x - μ_g||^2 + Tr(Σ_x + Σ_g - 2(Σ_xΣ_g)^{(1/2)}) $\n",
    "\n",
    "여기서:\n",
    "- $ μ_x $와 $ μ_g$는 각각 실제 데이터와 생성된 데이터의 특징 벡터의 평균입니다.\n",
    "- $Σ_x$와 $Σ_g$는 각각 실제 데이터와 생성된 데이터의 공분산 행렬입니다.\n",
    "- $Tr$은 행렬의 대각합(트레이스)을 의미합니다.\n",
    "- $(Σ_xΣ_g)^{(1/2)}$는 $Σ_x$와 $Σ_g$의 곱의 제곱근을 나타내며, 이 값은 공분산 행렬들의 유사성을 측정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/mseitzer/pytorch-fid\n",
    "\n",
    "# define frechet_distance\n",
    "def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1)\n",
    "            + np.trace(sigma2) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ 20241111 KCL test ------\n",
      "\n",
      "[FID]\n",
      "6.498\n"
     ]
    }
   ],
   "source": [
    "mu1 = output_activation.mean(axis=0)\n",
    "sigma1 = np.cov(output_activation, rowvar=False)\n",
    "\n",
    "mu2 = gt_activation.mean(axis=0)\n",
    "sigma2 = np.cov(gt_activation, rowvar=False)\n",
    "\n",
    "FID = frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "print('------ 20241111 KCL test ------\\n')\n",
    "print('[FID]')\n",
    "print('%.3f' % FID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGIST_VTON",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
